{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Deep learning has become a hype and everyone wants to try it for better model performance. However, the embarrassed thing is that the neural network sometimes (maybe a lot of times) is not better than a primitive linear model baseline, not even to mention about xgboost or lightgbm. One of the reasons is that neural network is sensitive to the input distributions and scales, which is not usually addressed when fitting linear models or tree-based models.\n",
    "\n",
    "Normalization (centred and scaled to unit variance) of input feature space should be able to fix the majority of the problem, but it canâ€™t help with deep neural network which could potentially shift the variation during training. Batch normalization layer is introduced to normalize the output from neural network layers to achieve a faster and stabler convergence.\n",
    "\n",
    "In this post, I will compare a neural network model with linear regression baseline and how batch normalization layer speeds up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# keras was used to build basic nerual network (multilayer perceptron)\n",
    "from keras.layers import Dense, BatchNormalization, Activation\n",
    "from keras.models import Input, Model, Sequential\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "# sklearn was used for train/valid split, linear model and regression metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics.regression import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Boston housing data (can be downloaded from http://lib.stat.cmu.edu/datasets/boston) was used in this demostration. I didn't find a good way to directly load this dataset, so some hacks were used to correctly parse the data.\n",
    "\n",
    "The columns are defined in the data dictionary:\n",
    "\n",
    "1. CRIM - per capita crime rate by town\n",
    "* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* INDUS - proportion of non-retail business acres per town.\n",
    "* CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "* NOX - nitric oxides concentration (parts per 10 million)\n",
    "* RM - average number of rooms per dwelling\n",
    "* AGE - proportion of owner-occupied units built prior to 1940\n",
    "* DIS - weighted distances to five Boston employment centres\n",
    "* RAD - index of accessibility to radial highways\n",
    "* TAX - full-value property-tax rate per `$10,000`\n",
    "* PTRATIO - pupil-teacher ratio by town\n",
    "* B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "* LSTAT - % lower status of the population\n",
    "* MEDV - Median value of owner-occupied homes in `$1000's`\n",
    "\n",
    "\n",
    "`MEDV` is the target variable we are going to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B1000</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO   B1000  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "res = pd.read_fwf('./boston.txt', skiprows = 22, header = None)\n",
    "rows_odd = res.loc[np.arange(0,len(res),2)].reset_index(drop = True)\n",
    "rows_even = res.loc[np.arange(1,len(res),2)].reset_index(drop = True)\n",
    "res = pd.concat([rows_odd, rows_even], axis=1).iloc[:,:14]\n",
    "res.columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS',\n",
    "               'RAD','TAX','PTRATIO','B1000','LSTAT','MEDV']\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B1000</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>1.716290</td>\n",
       "      <td>11.166008</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.696228</td>\n",
       "      <td>4.332016</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>2.653510</td>\n",
       "      <td>22.991219</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>1.999689</td>\n",
       "      <td>1.417166</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>0.585700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.081900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.073700</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.250895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.107300</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>2.326718</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.112625</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>9.966540</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>9.222900</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     1.716290   11.166008   11.136779    0.069170    0.554695    6.284634   \n",
       "std      2.653510   22.991219    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.081900    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.250895    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      2.326718   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max      9.966540   95.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO       B1000  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.696228    4.332016  408.237154   18.455534  356.674032   \n",
       "std     28.148861    1.999689    1.417166  168.537116    2.164946   91.294864   \n",
       "min      2.900000    0.585700    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.073700    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.107300    4.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.112625    5.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000    9.222900    8.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT        MEDV  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set: (404, 14)\n",
      "valid set: (102, 14)\n"
     ]
    }
   ],
   "source": [
    "train, valid = train_test_split(res, train_size = 0.8)\n",
    "print('train set: ' + str(train.shape))\n",
    "print('valid set: ' + str(valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.iloc[:,:13].values, train.iloc[:,13].values\n",
    "valid = valid.iloc[:,:13].values, valid.iloc[:,13].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression model â€“ Baseline\n",
    "\n",
    "Letâ€™s define function to score models using mean absolute error (MAE), mean squared error (MSE) and Pearson correlation, which can be used for subsequent models and save us some typing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear model => mae: 3.79, mse: 30.17, cor :0.84\n"
     ]
    }
   ],
   "source": [
    "def score_model(model, valid):\n",
    "    x, y = valid\n",
    "    preds = model.predict(x).reshape(-1)\n",
    "    label = y\n",
    "    mae = mean_absolute_error(label, preds)\n",
    "    mse = mean_squared_error(label, preds)\n",
    "    cor = pearsonr(label, preds)[0]\n",
    "    return mae, mse, cor\n",
    "\n",
    "# Then we simply train and evaluate vanilla linear model from sklearn.\n",
    "model_lm = LinearRegression()\n",
    "model_lm.fit(X = train[0], y = train[1])\n",
    "print('linear model => mae: %3.2f, mse: %3.2f, cor :%3.2f'%(\n",
    "    score_model(model_lm, valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple linear regression model without regularization. Validation mean absolute error is 3.79, which is baseline of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with single layer neural network\n",
    "Next, a single perceptron is used for a fair comparison with simple linear model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1211 09:03:10.532372 11044 deprecation_wrapper.py:119] From C:\\Users\\n174724\\.conda\\envs\\face2bmi\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1211 09:03:10.555372 11044 deprecation_wrapper.py:119] From C:\\Users\\n174724\\.conda\\envs\\face2bmi\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1211 09:03:10.559377 11044 deprecation_wrapper.py:119] From C:\\Users\\n174724\\.conda\\envs\\face2bmi\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "W1211 09:03:10.589373 11044 deprecation_wrapper.py:119] From C:\\Users\\n174724\\.conda\\envs\\face2bmi\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 14        \n",
      "=================================================================\n",
      "Total params: 14\n",
      "Trainable params: 14\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_mlp = Sequential()\n",
    "model_mlp.add(Dense(1, kernel_initializer='normal',\n",
    "                    activation='linear', input_dim = 13))\n",
    "model_mlp.compile('adam','mean_squared_error')\n",
    "model_mlp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first 200 epochs of training, the model performance is disappointingly low (MAE @ 5.08)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1211 09:03:10.801462 11044 deprecation_wrapper.py:119] From C:\\Users\\n174724\\.conda\\envs\\face2bmi\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W1211 09:03:11.023278 11044 deprecation_wrapper.py:119] From C:\\Users\\n174724\\.conda\\envs\\face2bmi\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp model => mae: 5.08, mse: 51.39, cor :0.73\n"
     ]
    }
   ],
   "source": [
    "model_mlp.fit(x = train[0], y = train[1], \n",
    "              epochs = 200, batch_size = 32, \n",
    "              verbose=0)\n",
    "print('mlp model => mae: %3.2f, mse: %3.2f, cor :%3.2f'%(\n",
    "    score_model(model_mlp, valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we fit another 2000 epochs, the score was getting closer the linear model baseline, which implies that the model is as capable as a linear model, but just the convergence is extremely slow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp model => mae: 3.77, mse: 32.89, cor :0.83\n"
     ]
    }
   ],
   "source": [
    "model_mlp.fit(x = train[0], y = train[1], \n",
    "              epochs = 2000, batch_size = 32, \n",
    "              verbose=0)\n",
    "print('mlp model => mae: %3.2f, mse: %3.2f, cor :%3.2f'%(\n",
    "    score_model(model_mlp, valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks with batch normalization\n",
    "Now, we define another two functions to add batch normalization layers after dense layers, to test different model scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(hidden_dim = None, bn = False):\n",
    " \n",
    "    input_layer = Input((13,), name = 'input')\n",
    " \n",
    "    if bn:   # add batch normlization layer\n",
    "        out = BatchNormalization()(input_layer)\n",
    "    else:\n",
    "        out = input_layer\n",
    " \n",
    "    if hidden_dim != None:\n",
    "        out = Dense(hidden_dim)(out)\n",
    "        if bn:\n",
    "            out = BatchNormalization()(out)\n",
    "        out = Activation('relu')(out)\n",
    " \n",
    "    out = Dense(1, kernel_initializer='normal',\n",
    "                activation='linear')(out)\n",
    "    model = Model(input_layer, out)\n",
    "    model.compile('adam','mean_squared_error',['mae'])\n",
    "    return model\n",
    " \n",
    "def train_model(model, train, valid, epochs, batch_size, model_name):\n",
    " \n",
    "    es = EarlyStopping(patience=5)\n",
    "    tb = TensorBoard(log_dir='./tensorboard/'+ model_name)\n",
    "    callbacks = [es, tb]\n",
    " \n",
    "    model.fit(x=train[0], y = train[1],\n",
    "              batch_size=batch_size,\n",
    "              verbose = 0,\n",
    "              epochs=epochs,\n",
    "              callbacks = callbacks,\n",
    "              shuffle=True,\n",
    "              validation_data = valid)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "letâ€™s run some scenarios to examine the model performance with fixed number of epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models for different scenarios\n",
    "model_mlp_baseline = build_mlp()    # baseline neural network\n",
    "model_mlp_bn = build_mlp(bn = True)   # neural network + batch normalization\n",
    "model_mlp_bn_h16 = build_mlp(hidden_dim=16, bn = True)    # extra hidden layer\n",
    "model_mlp_bn_h64 = build_mlp(hidden_dim=64, bn = True)    # extra wider hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1211 09:03:40.416682 11044 deprecation_wrapper.py:119] From C:\\Users\\n174724\\.conda\\envs\\face2bmi\\lib\\site-packages\\keras\\callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W1211 09:03:40.418684 11044 deprecation_wrapper.py:119] From C:\\Users\\n174724\\.conda\\envs\\face2bmi\\lib\\site-packages\\keras\\callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train all the models\n",
    "model_mlp_baseline = train_model(model_mlp_baseline, train, valid, epochs, batch_size, 'mlp_baseline')\n",
    "model_mlp_bn = train_model(model_mlp_bn, train, valid, epochs, batch_size, 'mlp_bn')\n",
    "model_mlp_bn_h16 = train_model(model_mlp_bn_h16, train, valid, epochs, batch_size, 'mlp_bn_h16')\n",
    "model_mlp_bn_h64 = train_model(model_mlp_bn_h64, train, valid, epochs, batch_size, 'mlp_bn_h64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp baseline model => mae: 5.79, mse: 69.34, cor :0.61\n",
      "mlp model with batch normalization => mae: 3.46, mse: 30.12, cor :0.84\n",
      "mlp model with batch normalization + extra hidden layer => mae: 3.11, mse: 20.40, cor :0.91\n",
      "mlp model with batch normalization + wider hidden layer => mae: 2.86, mse: 18.56, cor :0.92\n"
     ]
    }
   ],
   "source": [
    "# print model performance\n",
    "print('mlp baseline model => mae: %3.2f, mse: %3.2f, cor :%3.2f'%(\n",
    "score_model(model_mlp_baseline, valid)))\n",
    "print('mlp model with batch normalization => mae: %3.2f, mse: %3.2f, cor :%3.2f'%(\n",
    "score_model(model_mlp_bn, valid)))\n",
    "print('mlp model with batch normalization + extra hidden layer => mae: %3.2f, mse: %3.2f, cor :%3.2f'%(\n",
    "score_model(model_mlp_bn_h16, valid)))\n",
    "print('mlp model with batch normalization + wider hidden layer => mae: %3.2f, mse: %3.2f, cor :%3.2f'%(\n",
    "score_model(model_mlp_bn_h64, valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/6chaoran/data-story/master/deep-learning/batch_normalization/tensorboard_mae.png)\n",
    "\n",
    "With the linear model baseline, MAE@ 3.79:\n",
    "\n",
    "* NN baseline: premature stopped (because of early stop callback) with MAE @ 5.79\n",
    "* NN+ Batch Normalization: stably converged with MAE @ 3.46\n",
    "\n",
    "Increase the complexity of NN, on top with batch normalization:\n",
    "\n",
    "* NN+ BN + hidden 16-dimension layer: MAE @ 3.11\n",
    "* NN+ BN + hidden 64-dimension layer: MAE @ 2.86\n",
    "\n",
    "Just an additional hidden dense layer makes neural network outperforms the linear model, with help of batch normalization layers.\n",
    "\n",
    "To conclude, batch normalization clearly make the neural network  stable and itâ€™s essential for complex and deep neural network to speed up the model convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face2bmi",
   "language": "python",
   "name": "face2bmi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
